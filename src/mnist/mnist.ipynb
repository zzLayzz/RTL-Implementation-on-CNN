{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Without Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'cnn_env (Python 3.12.10)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'e:/Files/GitHub/FYP/cnn_env/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# Manually recreate the model architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and load weights manually\n",
    "model = create_model()\n",
    "loaded_model = tf.keras.models.load_model('mnist_trained_model1.h5')\n",
    "\n",
    "# Copy weights from loaded model to our manually created model\n",
    "for i, layer in enumerate(loaded_model.layers):\n",
    "    model.layers[i].set_weights(layer.get_weights())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print initial model information\n",
    "print(\"Original Model Summary:\")\n",
    "model.summary()\n",
    "model.fit(\n",
    "    train_images, train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_data=(test_images, test_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_model, test_accuracy_model = model.evaluate(test_images, test_labels)\n",
    "print(f\"\\nTest accuracy : {test_accuracy_model:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model after Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.9954 - loss: 0.0145 - val_accuracy: 0.9922 - val_loss: 0.0269\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 0.9912 - val_loss: 0.0287\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.9978 - loss: 0.0070 - val_accuracy: 0.9904 - val_loss: 0.0348\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9981 - loss: 0.0061 - val_accuracy: 0.9911 - val_loss: 0.0333\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9988 - loss: 0.0036 - val_accuracy: 0.9884 - val_loss: 0.0441\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9853 - loss: 0.0576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy after pruning: 0.9884\n",
      "\n",
      "Model Sparsity: 0.1618\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# Manually recreate the model architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and load weights manually\n",
    "model = create_model()\n",
    "loaded_model = tf.keras.models.load_model('mnist_trained_model1.h5')\n",
    "\n",
    "# Copy weights from loaded model to our manually created model\n",
    "for i, layer in enumerate(loaded_model.layers):\n",
    "    model.layers[i].set_weights(layer.get_weights())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Manual pruning approach\n",
    "def manual_prune_weights(weights, pruning_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Manually prune weights by setting low-magnitude weights to zero\n",
    "    \n",
    "    Args:\n",
    "    weights (numpy.ndarray): Weight matrix to prune\n",
    "    pruning_threshold (float): Threshold for pruning, based on absolute weight magnitude\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Pruned weight matrix\n",
    "    \"\"\"\n",
    "    # Calculate the threshold based on weight magnitudes\n",
    "    threshold = np.percentile(np.abs(weights), pruning_threshold * 100)\n",
    "    \n",
    "    # Set weights below threshold to zero\n",
    "    pruned_weights = weights.copy()\n",
    "    pruned_weights[np.abs(pruned_weights) < threshold] = 0\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "# Apply pruning to the model\n",
    "pruned_layers = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
    "        # Get current weights\n",
    "        weights = layer.get_weights()\n",
    "        \n",
    "        # Prune weights manually\n",
    "        pruned_weights = []\n",
    "        for weight_matrix in weights:\n",
    "            pruned_weights.append(manual_prune_weights(weight_matrix))\n",
    "        \n",
    "        # Set pruned weights\n",
    "        layer.set_weights(pruned_weights)\n",
    "    \n",
    "    pruned_layers.append(layer)\n",
    "\n",
    "# Recreate the model with pruned layers\n",
    "pruned_model = tf.keras.Sequential(pruned_layers)\n",
    "\n",
    "# Compile the pruned model\n",
    "pruned_model.compile(optimizer='adam', \n",
    "                     loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the pruned model\n",
    "pruned_model.fit(\n",
    "    train_images, train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_data=(test_images, test_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the pruned model\n",
    "test_loss, test_accuracy = pruned_model.evaluate(test_images, test_labels)\n",
    "print(f\"\\nTest accuracy after pruning: {test_accuracy:.4f}\")\n",
    "\n",
    "# Calculate model sparsity\n",
    "def calculate_model_sparsity(model):\n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "    for layer in model.layers:\n",
    "        for weight in layer.get_weights():\n",
    "            total_weights += np.prod(weight.shape)\n",
    "            zero_weights += np.sum(weight == 0)\n",
    "    \n",
    "    return zero_weights / total_weights\n",
    "\n",
    "sparsity = calculate_model_sparsity(pruned_model)\n",
    "print(f\"\\nModel Sparsity: {sparsity:.4f}\")\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model.save('mnist_pruned_model.h5')\n",
    "pruned_model.save('mnist_pruned_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmprj8ijhl7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmprj8ijhl7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmprj8ijhl7'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer_6')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1776207075792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776207069648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776207073104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776260925200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776260925008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776260927312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776260925968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1776260927696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the Keras model\n",
    "mnist_pruned_model = keras.models.load_model('mnist_pruned_model.h5')\n",
    "\n",
    "# Convert the trained model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mnist_pruned_model)\n",
    "\n",
    "# Enable optimizations for quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert the model\n",
    "quantized_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open(\"mnist_model_quantized.h5\", \"wb\") as f:\n",
    "    f.write(quantized_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
